<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Who is Adam?: The Question That Broke NeurIPS 2025 | Shirley Blogs</title>
<meta name="keywords" content="">
<meta name="description" content="A reviewer, tasked with evaluating a technical submission for NeurIPS 2025—the most prestigious AI conference in the world—left a comment that will go down in infamy:

&ldquo;Who is Adam?&rdquo;
For the uninitiated, asking &ldquo;Who is Adam?&rdquo; in a Deep Learning paper is akin to a mechanic asking &ldquo;What is a wheel?&rdquo; or a chef asking &ldquo;What is salt?&rdquo;
Adam (Adaptive Moment Estimation) is arguably the most popular optimization algorithm in modern deep learning. It has been the default optimizer for nearly a decade. It is not a person the authors forgot to cite. It is not an obscure character in the paper&rsquo;s narrative. It is the math that makes the models learn.">
<meta name="author" content="Shirley">
<link rel="canonical" href="https://chenzheruc.github.io/posts/20251206_adam/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chenzheruc.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenzheruc.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenzheruc.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenzheruc.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenzheruc.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chenzheruc.github.io/posts/20251206_adam/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZKM2YW9DPM"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-ZKM2YW9DPM');
        }
      </script><meta property="og:url" content="https://chenzheruc.github.io/posts/20251206_adam/">
  <meta property="og:site_name" content="Shirley Blogs">
  <meta property="og:title" content="Who is Adam?: The Question That Broke NeurIPS 2025">
  <meta property="og:description" content="A reviewer, tasked with evaluating a technical submission for NeurIPS 2025—the most prestigious AI conference in the world—left a comment that will go down in infamy:
“Who is Adam?”
For the uninitiated, asking “Who is Adam?” in a Deep Learning paper is akin to a mechanic asking “What is a wheel?” or a chef asking “What is salt?”
Adam (Adaptive Moment Estimation) is arguably the most popular optimization algorithm in modern deep learning. It has been the default optimizer for nearly a decade. It is not a person the authors forgot to cite. It is not an obscure character in the paper’s narrative. It is the math that makes the models learn.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-06T10:00:00+05:30">
    <meta property="article:modified_time" content="2025-12-06T10:00:00+05:30">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Who is Adam?: The Question That Broke NeurIPS 2025">
<meta name="twitter:description" content="A reviewer, tasked with evaluating a technical submission for NeurIPS 2025—the most prestigious AI conference in the world—left a comment that will go down in infamy:

&ldquo;Who is Adam?&rdquo;
For the uninitiated, asking &ldquo;Who is Adam?&rdquo; in a Deep Learning paper is akin to a mechanic asking &ldquo;What is a wheel?&rdquo; or a chef asking &ldquo;What is salt?&rdquo;
Adam (Adaptive Moment Estimation) is arguably the most popular optimization algorithm in modern deep learning. It has been the default optimizer for nearly a decade. It is not a person the authors forgot to cite. It is not an obscure character in the paper&rsquo;s narrative. It is the math that makes the models learn.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://chenzheruc.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Who is Adam?: The Question That Broke NeurIPS 2025",
      "item": "https://chenzheruc.github.io/posts/20251206_adam/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Who is Adam?: The Question That Broke NeurIPS 2025",
  "name": "Who is Adam?: The Question That Broke NeurIPS 2025",
  "description": "A reviewer, tasked with evaluating a technical submission for NeurIPS 2025—the most prestigious AI conference in the world—left a comment that will go down in infamy:\n\u0026ldquo;Who is Adam?\u0026rdquo;\nFor the uninitiated, asking \u0026ldquo;Who is Adam?\u0026rdquo; in a Deep Learning paper is akin to a mechanic asking \u0026ldquo;What is a wheel?\u0026rdquo; or a chef asking \u0026ldquo;What is salt?\u0026rdquo;\nAdam (Adaptive Moment Estimation) is arguably the most popular optimization algorithm in modern deep learning. It has been the default optimizer for nearly a decade. It is not a person the authors forgot to cite. It is not an obscure character in the paper\u0026rsquo;s narrative. It is the math that makes the models learn.\n",
  "keywords": [
    
  ],
  "articleBody": "A reviewer, tasked with evaluating a technical submission for NeurIPS 2025—the most prestigious AI conference in the world—left a comment that will go down in infamy:\n“Who is Adam?”\nFor the uninitiated, asking “Who is Adam?” in a Deep Learning paper is akin to a mechanic asking “What is a wheel?” or a chef asking “What is salt?”\nAdam (Adaptive Moment Estimation) is arguably the most popular optimization algorithm in modern deep learning. It has been the default optimizer for nearly a decade. It is not a person the authors forgot to cite. It is not an obscure character in the paper’s narrative. It is the math that makes the models learn.\nThat a reviewer at NeurIPS—a venue that received over 21,000 submissions this year—could ask this question is funny. But the laughter stops quickly when you realize what it actually implies: The peer review system for high-stakes ML conferences is cracking under the weight of AI-generated slop and unqualified reviewers.\nThe “Adam” Archetype: Hallucination or Incompetence? The “Who is Adam?” incident (and the resulting meme) highlights two distinct but equally damaging possibilities regarding the state of academic review in 2025:\n1. The “Lazy LLM” Review The leading theory is that the review wasn’t written by a human at all. The reviewer likely fed the paper into a Large Language Model (LLM) with a generic prompt like “Find weaknesses in this paper.”\nLLMs, despite being trained using Adam, can hallucinate “Adam” as an entity—specifically a person—when analyzing text structurally. If the paper mentions “We optimize using Adam,” a confused language model might flag “Adam” as an undefined proper noun if it lacks the specific context of that sentence.\nWe are seeing a flood of reviews that are “verbose but empty,” filled with bullet points that summarize the abstract rather than critique the method. “Who is Adam?” is just the most visible artifact of reviewers using AI to do their homework.\n2. The Unqualified Reviewer The second possibility is almost worse: The reviewer is a human, but one so junior or far removed from the field that they genuinely don’t know what the Adam optimizer is.\nWith submission numbers exploding (up nearly 100% since 2020), conferences are desperate for bodies. This leads to a frantic recruitment of reviewers, including undergraduates or researchers from adjacent fields (like pure statistics or biology) who may lack the specific vocabulary of modern Deep Learning.\nThe Consequence: High-Stakes Noise While we chuckle at the absurdity of asking who “Adam” is, the implications are anything but a joke. For a PhD student on the cusp of graduation, a rejection caused by such staggering incompetence can be catastrophic—delaying defenses, killing momentum, or forcing them to scrap years of valid work. The laughter on social media masks a grim reality: the gatekeepers of our most advanced science are failing, and the toll is being paid by young researchers whose futures depend on a fair hearing that they simply aren’t getting.\nIt is notorious that the quality of peer review in top computer science conferences has had deep-seated problems even without the interference of AI; the field has long battled issues of high variance, reviewer lottery, and inconsistent standards. However, the situation seems to be deteriorating rapidly with LLMs acting as the judge. We have moved from a system flawed by human bias and fatigue to one threatened by automated indifference, where the distinct voice of a critical expert is replaced by the smoothed-out, confident hallucinations of a model that knows the syntax of a review but none of the substance.\nFurthermore, this crisis of evaluation is not unique to academic peer review; it is a symptom of a broader collapse in how we scale judgment across complex systems. We see the exact same dysfunction in the promotion cycles of Big Tech companies, where engineers are often evaluated by committees far removed from their actual work, relying on gameable “impact metrics” rather than technical reality. It echoes in hiring pipelines where AI-generated resumes are filtered by AI-generated screeners, creating a feedback loop of noise. Whether it’s a “Promo Packet” or a “Conference Submission,” when the volume of content outpaces the availability of expert attention, the system defaults to heuristics, randomness, and increasingly, the hollow mimicry of intelligence provided by AI itself.\nCan We Fix the Machine? To truly fix the collapse of judgment systems due to scale, metric-gaming, and AI interference, we need to move beyond band-aids like “banning ChatGPT” and look at structural changes that address the root causes of noise and scale:\n1. “Skin in the Game” Mechanisms (Proof of Work) We must move away from “passive” reviewing to “active” verification. In conferences, this could mean reviewers must correctly answer three specific comprehension questions set by the authors (e.g., “What is the specific value of hyperparameter $\\alpha$?”) before their score is registered. This directly counters “automated indifference” because a lazy reviewer or an LLM cannot easily perform specific, context-heavy verification tasks without hallucinating.\n2. De-Scaling: The Federation Model Centralized systems handling 21,000 submissions inevitably default to noise. We should break the monolithic “NeurIPS” into smaller, high-trust, semi-autonomous tracks or sub-conferences that are capped in size. Acceptance is determined by local peers who actually know the work, allowing the “distinct voices” of experts to be heard over the statistical noise of a massive general pool.\n3. Cap-and-Trade for Submissions We need to artificially constrain the supply of content to match the available attention of experts. By limiting senior researchers to a fixed number of submissions per year (e.g., 3 slots as senior author), we force a “quality over quantity” filter at the source. If a researcher has limited slots, they won’t waste one on a half-baked paper that invites an AI-generated review, disrupting the feedback loop of noise.\nUntil then, if you submitted to NeurIPS 2025 and got a rejection, take a close look at the reviews. If they ask you who Adam is, just remember: It’s not you, it’s them.\n",
  "wordCount" : "992",
  "inLanguage": "en",
  "datePublished": "2025-12-06T10:00:00+05:30",
  "dateModified": "2025-12-06T10:00:00+05:30",
  "author":{
    "@type": "Person",
    "name": "Shirley"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chenzheruc.github.io/posts/20251206_adam/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shirley Blogs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chenzheruc.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenzheruc.github.io/" accesskey="h" title="Shirley Blogs (Alt + H)">Shirley Blogs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://chenzheruc.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Who is Adam?: The Question That Broke NeurIPS 2025
    </h1>
    <div class="post-meta"><span title='2025-12-06 10:00:00 +0530 +0530'>December 6, 2025</span>&nbsp;·&nbsp;<span>Shirley</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-adam-archetype-hallucination-or-incompetence" aria-label="The &ldquo;Adam&rdquo; Archetype: Hallucination or Incompetence?">The &ldquo;Adam&rdquo; Archetype: Hallucination or Incompetence?</a><ul>
                        
                <li>
                    <a href="#1-the-lazy-llm-review" aria-label="1. The &ldquo;Lazy LLM&rdquo; Review">1. The &ldquo;Lazy LLM&rdquo; Review</a></li>
                <li>
                    <a href="#2-the-unqualified-reviewer" aria-label="2. The Unqualified Reviewer">2. The Unqualified Reviewer</a></li></ul>
                </li>
                <li>
                    <a href="#the-consequence-high-stakes-noise" aria-label="The Consequence: High-Stakes Noise">The Consequence: High-Stakes Noise</a></li>
                <li>
                    <a href="#can-we-fix-the-machine" aria-label="Can We Fix the Machine?">Can We Fix the Machine?</a><ul>
                        
                <li>
                    <a href="#1-skin-in-the-game-mechanisms-proof-of-work" aria-label="1. &ldquo;Skin in the Game&rdquo; Mechanisms (Proof of Work)">1. &ldquo;Skin in the Game&rdquo; Mechanisms (Proof of Work)</a></li>
                <li>
                    <a href="#2-de-scaling-the-federation-model" aria-label="2. De-Scaling: The Federation Model">2. De-Scaling: The Federation Model</a></li>
                <li>
                    <a href="#3-cap-and-trade-for-submissions" aria-label="3. Cap-and-Trade for Submissions">3. Cap-and-Trade for Submissions</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>A reviewer, tasked with evaluating a technical submission for NeurIPS 2025—the most prestigious AI conference in the world—left a comment that will go down in infamy:</p>
<blockquote>
<p>&ldquo;Who is Adam?&rdquo;</p></blockquote>
<p>For the uninitiated, asking &ldquo;Who is Adam?&rdquo; in a Deep Learning paper is akin to a mechanic asking &ldquo;What is a wheel?&rdquo; or a chef asking &ldquo;What is salt?&rdquo;</p>
<p>Adam (Adaptive Moment Estimation) is arguably the most popular optimization algorithm in modern deep learning. It has been the default optimizer for nearly a decade. It is not a person the authors forgot to cite. It is not an obscure character in the paper&rsquo;s narrative. It is the math that makes the models learn.</p>
<p>That a reviewer at NeurIPS—a venue that received over 21,000 submissions this year—could ask this question is funny. But the laughter stops quickly when you realize what it actually implies: The peer review system for high-stakes ML conferences is cracking under the weight of AI-generated slop and unqualified reviewers.</p>
<h2 id="the-adam-archetype-hallucination-or-incompetence">The &ldquo;Adam&rdquo; Archetype: Hallucination or Incompetence?<a hidden class="anchor" aria-hidden="true" href="#the-adam-archetype-hallucination-or-incompetence">#</a></h2>
<p>The &ldquo;Who is Adam?&rdquo; incident (and the resulting meme) highlights two distinct but equally damaging possibilities regarding the state of academic review in 2025:</p>
<h3 id="1-the-lazy-llm-review">1. The &ldquo;Lazy LLM&rdquo; Review<a hidden class="anchor" aria-hidden="true" href="#1-the-lazy-llm-review">#</a></h3>
<p>The leading theory is that the review wasn&rsquo;t written by a human at all. The reviewer likely fed the paper into a Large Language Model (LLM) with a generic prompt like &ldquo;Find weaknesses in this paper.&rdquo;</p>
<p>LLMs, despite being trained using Adam, can hallucinate &ldquo;Adam&rdquo; as an entity—specifically a person—when analyzing text structurally. If the paper mentions &ldquo;We optimize using Adam,&rdquo; a confused language model might flag &ldquo;Adam&rdquo; as an undefined proper noun if it lacks the specific context of that sentence.</p>
<p>We are seeing a flood of reviews that are &ldquo;verbose but empty,&rdquo; filled with bullet points that summarize the abstract rather than critique the method. &ldquo;Who is Adam?&rdquo; is just the most visible artifact of reviewers using AI to do their homework.</p>
<h3 id="2-the-unqualified-reviewer">2. The Unqualified Reviewer<a hidden class="anchor" aria-hidden="true" href="#2-the-unqualified-reviewer">#</a></h3>
<p>The second possibility is almost worse: The reviewer is a human, but one so junior or far removed from the field that they genuinely don&rsquo;t know what the Adam optimizer is.</p>
<p>With submission numbers exploding (up nearly 100% since 2020), conferences are desperate for bodies. This leads to a frantic recruitment of reviewers, including undergraduates or researchers from adjacent fields (like pure statistics or biology) who may lack the specific vocabulary of modern Deep Learning.</p>
<h2 id="the-consequence-high-stakes-noise">The Consequence: High-Stakes Noise<a hidden class="anchor" aria-hidden="true" href="#the-consequence-high-stakes-noise">#</a></h2>
<p>While we chuckle at the absurdity of asking who &ldquo;Adam&rdquo; is, the implications are anything but a joke. For a PhD student on the cusp of graduation, a rejection caused by such staggering incompetence can be catastrophic—delaying defenses, killing momentum, or forcing them to scrap years of valid work. The laughter on social media masks a grim reality: the gatekeepers of our most advanced science are failing, and the toll is being paid by young researchers whose futures depend on a fair hearing that they simply aren&rsquo;t getting.</p>
<p>It is notorious that the quality of peer review in top computer science conferences has had deep-seated problems even without the interference of AI; the field has long battled issues of high variance, reviewer lottery, and inconsistent standards. However, the situation seems to be deteriorating rapidly with LLMs acting as the judge. We have moved from a system flawed by human bias and fatigue to one threatened by automated indifference, where the distinct voice of a critical expert is replaced by the smoothed-out, confident hallucinations of a model that knows the syntax of a review but none of the substance.</p>
<p>Furthermore, this crisis of evaluation is not unique to academic peer review; it is a symptom of a broader collapse in how we scale judgment across complex systems. We see the exact same dysfunction in the promotion cycles of Big Tech companies, where engineers are often evaluated by committees far removed from their actual work, relying on gameable &ldquo;impact metrics&rdquo; rather than technical reality. It echoes in hiring pipelines where AI-generated resumes are filtered by AI-generated screeners, creating a feedback loop of noise. Whether it&rsquo;s a &ldquo;Promo Packet&rdquo; or a &ldquo;Conference Submission,&rdquo; when the volume of content outpaces the availability of expert attention, the system defaults to heuristics, randomness, and increasingly, the hollow mimicry of intelligence provided by AI itself.</p>
<h2 id="can-we-fix-the-machine">Can We Fix the Machine?<a hidden class="anchor" aria-hidden="true" href="#can-we-fix-the-machine">#</a></h2>
<p>To truly fix the  collapse of judgment systems due to scale, metric-gaming, and AI interference, we need to move beyond band-aids like &ldquo;banning ChatGPT&rdquo; and look at structural changes that address the root causes of noise and scale:</p>
<h3 id="1-skin-in-the-game-mechanisms-proof-of-work">1. &ldquo;Skin in the Game&rdquo; Mechanisms (Proof of Work)<a hidden class="anchor" aria-hidden="true" href="#1-skin-in-the-game-mechanisms-proof-of-work">#</a></h3>
<p>We must move away from &ldquo;passive&rdquo; reviewing to &ldquo;active&rdquo; verification. In conferences, this could mean reviewers must correctly answer three specific comprehension questions set by the authors (e.g., &ldquo;What is the specific value of hyperparameter $\alpha$?&rdquo;) before their score is registered. This directly counters &ldquo;automated indifference&rdquo; because a lazy reviewer or an LLM cannot easily perform specific, context-heavy verification tasks without hallucinating.</p>
<h3 id="2-de-scaling-the-federation-model">2. De-Scaling: The Federation Model<a hidden class="anchor" aria-hidden="true" href="#2-de-scaling-the-federation-model">#</a></h3>
<p>Centralized systems handling 21,000 submissions inevitably default to noise. We should break the monolithic &ldquo;NeurIPS&rdquo; into smaller, high-trust, semi-autonomous tracks or sub-conferences that are capped in size. Acceptance is determined by local peers who actually know the work, allowing the &ldquo;distinct voices&rdquo; of experts to be heard over the statistical noise of a massive general pool.</p>
<h3 id="3-cap-and-trade-for-submissions">3. Cap-and-Trade for Submissions<a hidden class="anchor" aria-hidden="true" href="#3-cap-and-trade-for-submissions">#</a></h3>
<p>We need to artificially constrain the supply of content to match the available attention of experts. By limiting senior researchers to a fixed number of submissions per year (e.g., 3 slots as senior author), we force a &ldquo;quality over quantity&rdquo; filter at the source. If a researcher has limited slots, they won&rsquo;t waste one on a half-baked paper that invites an AI-generated review, disrupting the feedback loop of noise.</p>
<p>Until then, if you submitted to NeurIPS 2025 and got a rejection, take a close look at the reviews. If they ask you who Adam is, just remember: It’s not you, it’s them.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://chenzheruc.github.io/">Shirley Blogs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
