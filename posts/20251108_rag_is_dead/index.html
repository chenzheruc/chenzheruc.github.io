<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The Twilight of RAG: How LLM In-Context Ranking is Rewriting the Rules | Shirley Blogs</title>
<meta name="keywords" content="">
<meta name="description" content="For the past few years, Retrieval-Augmented Generation (RAG) has been the cornerstone of scaling Large Language Models (LLMs) to massive knowledge bases. Since early LLMs suffered from limited input length—with models like GPT-4 handling only about 8,192 tokens (roughly 12 pages)—RAG provided an elegant, if complex, workaround: retrieve the most relevant fragments and feed those to the LLM.
However, the rapid evolution of LLMs and their specialized ranking capabilities, combined with exploding context windows, suggests that the traditional RAG architecture we built and optimized is fundamentally on the decline.">
<meta name="author" content="Shirley">
<link rel="canonical" href="https://chenzheruc.github.io/posts/20251108_rag_is_dead/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chenzheruc.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenzheruc.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenzheruc.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenzheruc.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenzheruc.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chenzheruc.github.io/posts/20251108_rag_is_dead/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZKM2YW9DPM"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-ZKM2YW9DPM');
        }
      </script><meta property="og:url" content="https://chenzheruc.github.io/posts/20251108_rag_is_dead/">
  <meta property="og:site_name" content="Shirley Blogs">
  <meta property="og:title" content="The Twilight of RAG: How LLM In-Context Ranking is Rewriting the Rules">
  <meta property="og:description" content="For the past few years, Retrieval-Augmented Generation (RAG) has been the cornerstone of scaling Large Language Models (LLMs) to massive knowledge bases. Since early LLMs suffered from limited input length—with models like GPT-4 handling only about 8,192 tokens (roughly 12 pages)—RAG provided an elegant, if complex, workaround: retrieve the most relevant fragments and feed those to the LLM. However, the rapid evolution of LLMs and their specialized ranking capabilities, combined with exploding context windows, suggests that the traditional RAG architecture we built and optimized is fundamentally on the decline.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-11-08T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-11-08T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Twilight of RAG: How LLM In-Context Ranking is Rewriting the Rules">
<meta name="twitter:description" content="For the past few years, Retrieval-Augmented Generation (RAG) has been the cornerstone of scaling Large Language Models (LLMs) to massive knowledge bases. Since early LLMs suffered from limited input length—with models like GPT-4 handling only about 8,192 tokens (roughly 12 pages)—RAG provided an elegant, if complex, workaround: retrieve the most relevant fragments and feed those to the LLM.
However, the rapid evolution of LLMs and their specialized ranking capabilities, combined with exploding context windows, suggests that the traditional RAG architecture we built and optimized is fundamentally on the decline.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://chenzheruc.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The Twilight of RAG: How LLM In-Context Ranking is Rewriting the Rules",
      "item": "https://chenzheruc.github.io/posts/20251108_rag_is_dead/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The Twilight of RAG: How LLM In-Context Ranking is Rewriting the Rules",
  "name": "The Twilight of RAG: How LLM In-Context Ranking is Rewriting the Rules",
  "description": "For the past few years, Retrieval-Augmented Generation (RAG) has been the cornerstone of scaling Large Language Models (LLMs) to massive knowledge bases. Since early LLMs suffered from limited input length—with models like GPT-4 handling only about 8,192 tokens (roughly 12 pages)—RAG provided an elegant, if complex, workaround: retrieve the most relevant fragments and feed those to the LLM. However, the rapid evolution of LLMs and their specialized ranking capabilities, combined with exploding context windows, suggests that the traditional RAG architecture we built and optimized is fundamentally on the decline.\n",
  "keywords": [
    
  ],
  "articleBody": "For the past few years, Retrieval-Augmented Generation (RAG) has been the cornerstone of scaling Large Language Models (LLMs) to massive knowledge bases. Since early LLMs suffered from limited input length—with models like GPT-4 handling only about 8,192 tokens (roughly 12 pages)—RAG provided an elegant, if complex, workaround: retrieve the most relevant fragments and feed those to the LLM. However, the rapid evolution of LLMs and their specialized ranking capabilities, combined with exploding context windows, suggests that the traditional RAG architecture we built and optimized is fundamentally on the decline.\nRelease Date Model Family Model Name Max Context Tokens Nov 30, 2022 OpenAI GPT-3.5 Turbo 16,385 (16K) Mar 14, 2023 OpenAI GPT-4 8,192 (8K) Sep 27, 2023 Mistral Mistral 7B 32,768 (32K) Nov 6, 2023 OpenAI GPT-4 Turbo 128,000 (128K) Mar 4, 2024 Anthropic Claude 3 (Opus, Sonnet, Haiku) 200,000 (200K) Apr 18, 2024 Meta Llama 3 8,192 (8K) May 13, 2024 OpenAI GPT-4o 128,000 (128K) Jun 20, 2024 Anthropic Claude 3.5 Sonnet 200,000 (200K) Jul 23, 2024 Meta Llama 3.1 128,000 (128K) Jul 24, 2024 Mistral Mistral Large 2 128,000 (128K) Jun 17, 2025 Google Gemini 2.5 Pro 1,048,576 (1M) This table is automatically generated by Gemini 2.5 Pro. The table shows a clear and dramatic trend: the exponential growth of LLM context windows, driven by intense and rapid competition. In less than three years, the standard for flagship models has exploded from 16K tokens (GPT-3.5) to a competitive benchmark of 128K-200K in 2024 across all major labs. This race immediately escalated into a new “million-token” era in mid-2025, with Google’s Gemini 2.5 series setting a 1M token precedent, demonstrating that massive context size has become a critical and rapidly advancing frontier for model capability. The trajectory is even more dramatic: we’re likely heading toward 10M+ context windows by 2027, with Sam Altman hinting at billions of context tokens on the horizon.\nAs a result, the future is shifting away from fragmented retrieval pipelines and moving toward highly efficient, comprehensive LLM ranking and intelligent navigation.\nThe Unbearable Burden of Traditional RAG RAG was a brilliant band-aid, but it relies on a complex, multi-step pipeline fraught with points of failure and computational bottlenecks:\nThe Chunking Challenge: Long documents must be broken into digestible pieces, typically 400-1,000 tokens. Even with sophisticated techniques to preserve hierarchical structure and table integrity, chunking ultimately destroys context and semantic relationships permanently, leading to fragmented understanding. The Hybrid Search Nightmare: Achieving accurate retrieval required combining keyword search (BM25) with semantic search (embeddings). This necessitates sophisticated engineering for parallel processing, dynamic weighting, and score normalization using methods like Reciprocal Rank Fusion (RRF). The Reranking Bottleneck: After all that work, an expensive second step—reranking—was necessary to limit the number of chunks sent to the context-poor LLM. This adds significant latency (300ms to 2000ms per query) and increases API costs. This multi-stage process results in a “cascading failure problem,” where errors compound from chunking to embedding to fusion to reranking. RAG treats long documents as independent paragraphs, fundamentally failing on complex analysis that requires causal understanding and tracing cross-references. In-context Ranking: An Emerging Paradigm In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR) that utilizes the comprehensive contextual understanding capabilities of Large Language Models (LLMs) to perform listwise ranking. In this setup, the LLM is directly tasked with processing a list of candidate documents and a query simultaneously to determine relevance and output a ranked list. There are active research in this area, for example:\nLATTICE is introduced as a training-free, LLM-guided hierarchical retrieval framework specifically engineered for complex, reasoning-intensive queries, offering an alternative to the limitations of retrieve-then-rerank and long-context paradigms. The system organizes the document corpus into a semantic tree offline, a process achieved through LLM-driven strategies like bottom-up clustering or top-down divisive summarization. This hierarchy allows the framework to achieve search complexity that is logarithmic in the number of documents. During online query processing, a “LLM” navigates this structure using a greedy, best-first traversal. A crucial component of the traversal algorithm is the estimation of calibrated latent relevance scores, which are aggregated into a path relevance metric to reliably guide the search across different branches and levels, mitigating the challenge of noisy, context-dependent LLM relevance judgments.\nThis paper presents a comprehensive study on utilizing long-context Large Language Models (LLMs) for listwise passage ranking, primarily contrasting the traditional, inefficient sliding window strategy with a novel, more efficient full ranking strategy. The sliding window approach, used historically due to limited context length, incurs redundant API costs and serialized processing; in contrast, full ranking processes all passages in a single inference step, resulting in superior efficiency and roughly 50% lower API costs. While the full ranking strategy exhibited higher efficiency but lower effectiveness in a zero-shot setting, the paper reveals that in the supervised fine-tuning setting, the full ranking model achieves superior performance compared to the sliding window model. To effectively fine-tune the full ranking model, the authors propose two key innovations: a multi-pass sliding window approach to generate a complete listwise label, and an importance-aware learning objective that assigns greater weight to top-ranked passage IDs during loss calculation, ensuring the fine-tuned full ranking model outperforms baselines in both ranking effectiveness and efficiency.\nThe paper presents BlockRank (Blockwise In-context Ranking), a novel and efficient method developed to address the significant efficiency challenges associated with using generative Large Language Models (LLMs) for In-context Ranking (ICR), particularly the quadratic scaling of attention with increasing context length. BlockRank is motivated by an analysis of LLMs fine-tuned for ICR, which revealed inherent structures in attention patterns: inter-document block sparsity (document tokens focus locally) and query-document block relevance (specific query tokens develop strong retrieval signals toward relevant documents in middle layers). Building on these insights, BlockRank introduces two key modifications: (1) a structured sparse attention mechanism that architecturally enforces sparsity, successfully reducing the attention complexity from quadratic to linear concerning the number of documents (N); and (2) an auxiliary contrastive learning objective applied at a middle layer to explicitly optimize these internal attention scores to reflect relevance. This optimization enables a highly efficient attention-based inference method that bypasses iterative auto-regressive decoding\nConclusion: The Post-Retrieval Age is Here RAG was an indispensable training wheel for the context-poor era. It allowed us to solve problems that exceeded the short attention span of early LLMs. Now, however, we have crossed a threshold. We are entering the post-retrieval age where LLM-native approaches offer superior performance, lower cost, and streamlined infrastructure:\nRAG (The Old Way) LLM-Native Ranking (The Future) Fragmented chunking Full document context/Hierarchical summaries Latency and cost multiplication Linear scaling, massive efficiency gains Relies on similarity (embeddings) Relies on precision and reasoning Complex, brittle pipeline Simple, agentic navigation The shift is clear: the LLM is no longer just a summarizer waiting for fragments; it is the core ranking engine and the intelligent search agent. The cumbersome infrastructure of RAG is being replaced by the sheer power and efficiency of LLM ranking and reasoning in abundant context.\nReferences The RAG Obituary: Killed by Agents, Buried by Context Windows LLM-Guided Hierarchical Retrieval Scalable In-context Ranking with Generative Models Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models ",
  "wordCount" : "1193",
  "inLanguage": "en",
  "datePublished": "2025-11-08T00:00:00Z",
  "dateModified": "2025-11-08T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Shirley"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chenzheruc.github.io/posts/20251108_rag_is_dead/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shirley Blogs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chenzheruc.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenzheruc.github.io/" accesskey="h" title="Shirley Blogs (Alt + H)">Shirley Blogs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://chenzheruc.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      The Twilight of RAG: How LLM In-Context Ranking is Rewriting the Rules
    </h1>
    <div class="post-meta"><span title='2025-11-08 00:00:00 +0000 UTC'>November 8, 2025</span>&nbsp;·&nbsp;<span>Shirley</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#the-unbearable-burden-of-traditional-rag" aria-label="The Unbearable Burden of Traditional RAG">The Unbearable Burden of Traditional RAG</a></li>
                <li>
                    <a href="#in-context-ranking-an-emerging-paradigm" aria-label="In-context Ranking: An Emerging Paradigm">In-context Ranking: An Emerging Paradigm</a></li>
                <li>
                    <a href="#conclusion-the-post-retrieval-age-is-here" aria-label="Conclusion: The Post-Retrieval Age is Here">Conclusion: The Post-Retrieval Age is Here</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>For the past few years, Retrieval-Augmented Generation (RAG) has been the cornerstone of scaling Large Language Models (LLMs) to massive knowledge bases. Since early LLMs suffered from limited input length—with models like GPT-4 handling only about 8,192 tokens (roughly 12 pages)—RAG provided an elegant, if complex, workaround: retrieve the most relevant fragments and feed those to the LLM.
However, the rapid evolution of LLMs and their specialized ranking capabilities, combined with exploding context windows, suggests that the traditional RAG architecture we built and optimized is fundamentally on the decline.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">Release Date</th>
          <th style="text-align: left">Model Family</th>
          <th style="text-align: left">Model Name</th>
          <th style="text-align: left">Max Context Tokens</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Nov 30, 2022</td>
          <td style="text-align: left"><strong>OpenAI</strong></td>
          <td style="text-align: left"><strong>GPT-3.5 Turbo</strong></td>
          <td style="text-align: left"><strong>16,385</strong> (16K)</td>
      </tr>
      <tr>
          <td style="text-align: left">Mar 14, 2023</td>
          <td style="text-align: left"><strong>OpenAI</strong></td>
          <td style="text-align: left"><strong>GPT-4</strong></td>
          <td style="text-align: left">8,192 (8K)</td>
      </tr>
      <tr>
          <td style="text-align: left">Sep 27, 2023</td>
          <td style="text-align: left"><strong>Mistral</strong></td>
          <td style="text-align: left">Mistral 7B</td>
          <td style="text-align: left">32,768 (32K)</td>
      </tr>
      <tr>
          <td style="text-align: left">Nov 6, 2023</td>
          <td style="text-align: left"><strong>OpenAI</strong></td>
          <td style="text-align: left"><strong>GPT-4 Turbo</strong></td>
          <td style="text-align: left"><strong>128,000</strong> (128K)</td>
      </tr>
      <tr>
          <td style="text-align: left">Mar 4, 2024</td>
          <td style="text-align: left"><strong>Anthropic</strong></td>
          <td style="text-align: left">Claude 3 (Opus, Sonnet, Haiku)</td>
          <td style="text-align: left">200,000 (200K)</td>
      </tr>
      <tr>
          <td style="text-align: left">Apr 18, 2024</td>
          <td style="text-align: left"><strong>Meta</strong></td>
          <td style="text-align: left">Llama 3</td>
          <td style="text-align: left">8,192 (8K)</td>
      </tr>
      <tr>
          <td style="text-align: left">May 13, 2024</td>
          <td style="text-align: left"><strong>OpenAI</strong></td>
          <td style="text-align: left"><strong>GPT-4o</strong></td>
          <td style="text-align: left"><strong>128,000</strong> (128K)</td>
      </tr>
      <tr>
          <td style="text-align: left">Jun 20, 2024</td>
          <td style="text-align: left"><strong>Anthropic</strong></td>
          <td style="text-align: left"><strong>Claude 3.5 Sonnet</strong></td>
          <td style="text-align: left"><strong>200,000</strong> (200K)</td>
      </tr>
      <tr>
          <td style="text-align: left">Jul 23, 2024</td>
          <td style="text-align: left"><strong>Meta</strong></td>
          <td style="text-align: left"><strong>Llama 3.1</strong></td>
          <td style="text-align: left"><strong>128,000</strong> (128K)</td>
      </tr>
      <tr>
          <td style="text-align: left">Jul 24, 2024</td>
          <td style="text-align: left"><strong>Mistral</strong></td>
          <td style="text-align: left"><strong>Mistral Large 2</strong></td>
          <td style="text-align: left"><strong>128,000</strong> (128K)</td>
      </tr>
      <tr>
          <td style="text-align: left">Jun 17, 2025</td>
          <td style="text-align: left"><strong>Google</strong></td>
          <td style="text-align: left"><strong>Gemini 2.5 Pro</strong></td>
          <td style="text-align: left"><strong>1,048,576</strong> (1M)</td>
      </tr>
  </tbody>
</table>
<ul>
<li>This table is automatically generated by Gemini 2.5 Pro.</li>
</ul>
<p>The table shows a clear and dramatic trend: the exponential growth of LLM context windows, driven by intense and rapid competition. In less than three years, the standard for flagship models has exploded from 16K tokens (GPT-3.5) to a competitive benchmark of 128K-200K in 2024 across all major labs. This race immediately escalated into a new &ldquo;million-token&rdquo; era in mid-2025, with Google&rsquo;s Gemini 2.5 series setting a 1M token precedent, demonstrating that massive context size has become a critical and rapidly advancing frontier for model capability.
The trajectory is even more dramatic: we’re likely heading toward 10M+ context windows by 2027, with Sam Altman hinting at billions of context tokens on the horizon.</p>
<p>As a result, the future is shifting away from fragmented retrieval pipelines and moving toward highly efficient, comprehensive LLM ranking and intelligent navigation.</p>
<h2 id="the-unbearable-burden-of-traditional-rag">The Unbearable Burden of Traditional RAG<a hidden class="anchor" aria-hidden="true" href="#the-unbearable-burden-of-traditional-rag">#</a></h2>
<p>RAG was a brilliant band-aid, but it relies on a complex, multi-step pipeline fraught with points of <a href="https://www.nicolasbustamante.com/p/the-rag-obituary-killed-by-agents">failure and computational bottlenecks</a>:</p>
<ul>
<li><strong>The Chunking Challenge</strong>: Long documents must be broken into digestible pieces, typically 400-1,000 tokens. Even with sophisticated techniques to preserve hierarchical structure and table integrity, chunking ultimately destroys context and semantic relationships permanently, leading to fragmented understanding.</li>
<li><strong>The Hybrid Search Nightmare</strong>: Achieving accurate retrieval required combining keyword search (BM25) with semantic search (embeddings). This necessitates sophisticated engineering for parallel processing, dynamic weighting, and score normalization using methods like Reciprocal Rank Fusion (RRF).</li>
<li><strong>The Reranking Bottleneck</strong>: After all that work, an expensive second step—reranking—was necessary to limit the number of chunks sent to the context-poor LLM. This adds significant latency (300ms to 2000ms per query) and increases API costs.
This multi-stage process results in a &ldquo;cascading failure problem,&rdquo; where errors compound from chunking to embedding to fusion to reranking. RAG treats long documents as independent paragraphs, fundamentally failing on complex analysis that requires causal understanding and tracing cross-references.</li>
</ul>
<h2 id="in-context-ranking-an-emerging-paradigm">In-context Ranking: An Emerging Paradigm<a hidden class="anchor" aria-hidden="true" href="#in-context-ranking-an-emerging-paradigm">#</a></h2>
<p>In-context Ranking (ICR) is <a href="https://arxiv.org/pdf/2510.05396?">an emerging paradigm</a> for Information Retrieval (IR) that utilizes the comprehensive contextual understanding capabilities of Large Language Models (LLMs) to perform listwise ranking.
In this setup, the LLM is directly tasked with processing a list of candidate documents and a query simultaneously to determine relevance and output a ranked list. There are active research in this area, for example:</p>
<ul>
<li>
<p><a href="https://arxiv.org/pdf/2510.13217">LATTICE</a> is introduced as a training-free, LLM-guided hierarchical retrieval framework specifically engineered for complex, reasoning-intensive queries, offering an alternative to the limitations of retrieve-then-rerank and long-context paradigms. The system organizes the document corpus into a semantic tree offline, a process achieved through LLM-driven strategies like bottom-up clustering or top-down divisive summarization. This hierarchy allows the framework to achieve search complexity that is logarithmic in the number of documents. During online query processing, a &ldquo;LLM&rdquo; navigates this structure using a greedy, best-first traversal. A crucial component of the traversal algorithm is the estimation of calibrated latent relevance scores, which are aggregated into a path relevance metric to reliably guide the search across different branches and levels, mitigating the challenge of noisy, context-dependent LLM relevance judgments.</p>
</li>
<li>
<p>This <a href="https://arxiv.org/abs/2412.14574">paper</a> presents a comprehensive study on utilizing long-context Large Language Models (LLMs) for listwise passage ranking, primarily contrasting the traditional, inefficient sliding window strategy with a novel, more efficient full ranking strategy. The sliding window approach, used historically due to limited context length, incurs redundant API costs and serialized processing; in contrast, full ranking processes all passages in a single inference step, resulting in superior efficiency and roughly 50% lower API costs. While the full ranking strategy exhibited higher efficiency but lower effectiveness in a zero-shot setting, the paper reveals that in the supervised fine-tuning setting, the full ranking model achieves superior performance compared to the sliding window model. To effectively fine-tune the full ranking model, the authors propose two key innovations: a multi-pass sliding window approach to generate a complete listwise label, and an importance-aware learning objective that assigns greater weight to top-ranked passage IDs during loss calculation, ensuring the fine-tuned full ranking model outperforms baselines in both ranking effectiveness and efficiency.</p>
</li>
<li>
<p>The <a href="https://arxiv.org/pdf/2510.05396?">paper</a> presents BlockRank (Blockwise In-context Ranking), a novel and efficient method developed to address the significant efficiency challenges associated with using generative Large Language Models (LLMs) for In-context Ranking (ICR), particularly the quadratic scaling of attention with increasing context length. BlockRank is motivated by an analysis of LLMs fine-tuned for ICR, which revealed inherent structures in attention patterns: inter-document block sparsity (document tokens focus locally) and query-document block relevance (specific query tokens develop strong retrieval signals toward relevant documents in middle layers). Building on these insights, BlockRank introduces two key modifications: (1) a structured sparse attention mechanism that architecturally enforces sparsity, successfully reducing the attention complexity from quadratic to linear concerning the number of documents (N); and (2) an auxiliary contrastive learning objective applied at a middle layer to explicitly optimize these internal attention scores to reflect relevance. This optimization enables a highly efficient attention-based inference method that bypasses iterative auto-regressive decoding</p>
</li>
</ul>
<h2 id="conclusion-the-post-retrieval-age-is-here">Conclusion: The Post-Retrieval Age is Here<a hidden class="anchor" aria-hidden="true" href="#conclusion-the-post-retrieval-age-is-here">#</a></h2>
<p>RAG was an indispensable training wheel for the context-poor era. It allowed us to solve problems that exceeded the short attention span of early LLMs.
Now, however, we have crossed a threshold. We are entering the post-retrieval age where LLM-native approaches offer superior performance, lower cost, and streamlined infrastructure:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: left">RAG (The Old Way)</th>
          <th style="text-align: left">LLM-Native Ranking (The Future)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: left">Fragmented chunking</td>
          <td style="text-align: left">Full document context/Hierarchical summaries</td>
      </tr>
      <tr>
          <td style="text-align: left">Latency and cost multiplication</td>
          <td style="text-align: left">Linear scaling, massive efficiency gains</td>
      </tr>
      <tr>
          <td style="text-align: left">Relies on similarity (embeddings)</td>
          <td style="text-align: left">Relies on precision and reasoning</td>
      </tr>
      <tr>
          <td style="text-align: left">Complex, brittle pipeline</td>
          <td style="text-align: left">Simple, agentic navigation</td>
      </tr>
  </tbody>
</table>
<p>The shift is clear: the LLM is no longer just a summarizer waiting for fragments; it is the core ranking engine and the intelligent search agent. The cumbersome infrastructure of RAG is being replaced by the sheer power and efficiency of LLM ranking and reasoning in abundant context.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li><a href="https://www.nicolasbustamante.com/p/the-rag-obituary-killed-by-agents">The RAG Obituary: Killed by Agents, Buried by Context Windows</a></li>
<li><a href="https://arxiv.org/pdf/2510.13217">LLM-Guided Hierarchical Retrieval</a></li>
<li><a href="https://arxiv.org/pdf/2510.05396?">Scalable In-context Ranking with Generative Models</a></li>
<li><a href="https://arxiv.org/abs/2412.14574">Sliding Windows Are Not the End: Exploring Full Ranking with Long-Context Large Language Models</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://chenzheruc.github.io/">Shirley Blogs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
