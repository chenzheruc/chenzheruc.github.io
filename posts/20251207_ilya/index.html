<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The End of Guaranteed Win: Why Ilya Sutskever Says We&#39;re Entering the Age of Discovery | Shirley Blogs</title>
<meta name="keywords" content="">
<meta name="description" content="
For the last five years, the recipe for AI progress has been remarkably simple: take a neural network, add more data, add more compute, and watch the intelligence line go up. This was the Age of Scaling. It was an era of industrial certainty, where companies could pour billions into hardware with the confidence of a guaranteed return.
But according to Ilya Sutskever, that era is drawing to a close.">
<meta name="author" content="Shirley">
<link rel="canonical" href="https://chenzheruc.github.io/posts/20251207_ilya/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chenzheruc.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenzheruc.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenzheruc.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenzheruc.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenzheruc.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chenzheruc.github.io/posts/20251207_ilya/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZKM2YW9DPM"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-ZKM2YW9DPM');
        }
      </script><meta property="og:url" content="https://chenzheruc.github.io/posts/20251207_ilya/">
  <meta property="og:site_name" content="Shirley Blogs">
  <meta property="og:title" content="The End of Guaranteed Win: Why Ilya Sutskever Says We&#39;re Entering the Age of Discovery">
  <meta property="og:description" content="
For the last five years, the recipe for AI progress has been remarkably simple: take a neural network, add more data, add more compute, and watch the intelligence line go up. This was the Age of Scaling. It was an era of industrial certainty, where companies could pour billions into hardware with the confidence of a guaranteed return.
But according to Ilya Sutskever, that era is drawing to a close.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-07T12:00:00+05:30">
    <meta property="article:modified_time" content="2025-12-07T12:00:00+05:30">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The End of Guaranteed Win: Why Ilya Sutskever Says We&#39;re Entering the Age of Discovery">
<meta name="twitter:description" content="
For the last five years, the recipe for AI progress has been remarkably simple: take a neural network, add more data, add more compute, and watch the intelligence line go up. This was the Age of Scaling. It was an era of industrial certainty, where companies could pour billions into hardware with the confidence of a guaranteed return.
But according to Ilya Sutskever, that era is drawing to a close.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://chenzheruc.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The End of Guaranteed Win: Why Ilya Sutskever Says We're Entering the Age of Discovery",
      "item": "https://chenzheruc.github.io/posts/20251207_ilya/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The End of Guaranteed Win: Why Ilya Sutskever Says We're Entering the Age of Discovery",
  "name": "The End of Guaranteed Win: Why Ilya Sutskever Says We\u0027re Entering the Age of Discovery",
  "description": "\nFor the last five years, the recipe for AI progress has been remarkably simple: take a neural network, add more data, add more compute, and watch the intelligence line go up. This was the Age of Scaling. It was an era of industrial certainty, where companies could pour billions into hardware with the confidence of a guaranteed return.\nBut according to Ilya Sutskever, that era is drawing to a close.\n",
  "keywords": [
    
  ],
  "articleBody": "\nFor the last five years, the recipe for AI progress has been remarkably simple: take a neural network, add more data, add more compute, and watch the intelligence line go up. This was the Age of Scaling. It was an era of industrial certainty, where companies could pour billions into hardware with the confidence of a guaranteed return.\nBut according to Ilya Sutskever, that era is drawing to a close.\nIn a recent interview with Dwarkesh Patel, Sutskever argued that we are leaving the Age of Scaling (roughly 2020–2025) and entering the Age of Discovery (or Research). Here is why the rules of the game are changing, and what that means for the future of AI.\nThe Age of Scaling: When “More” Was Enough Sutskever characterizes the period between 2020 and 2025 as a time when a single insight—pre-training—sucked the air out of the room.\nThe “scaling hypothesis” wasn’t just a theory; it was a business plan. It told executives exactly what to do: “If you mix some compute with some data into a neural net of a certain size, you will get results.” It was low-risk and high-reward. Because everyone knew the recipe worked, execution became more important than ideation.\n“Scaling sucked out all the air in the room,” Sutskever noted. “We got to the point where there are more companies than ideas.”\nBut this strategy relied on a resource that is now dwindling: pre-training data. The internet is finite. We have scraped the text of humanity, and while synthetic data and “souped-up” pre-training offer some runway, the era of simply 100x-ing the dataset to get a 100x smarter model is effectively over.\nThe Competitive Programmer vs. The Natural Talent To explain why current models fall short despite their massive scale, Sutskever offered a striking analogy involving two students:\nStudent A (The Model): Wants to be the best competitive programmer. They practice for 10,000 hours, memorize every proof technique, and solve every problem ever written. They become a machine at coding competitions.\nStudent B (The Human): Practices for 100 hours. They have the “it” factor—a deep, intuitive understanding of the underlying logic.\nCurrently, LLMs are Student A. They have “memorized” the internet’s worth of patterns (the 10,000 hours), but they lack the fundamental “it” factor that allows Student B to generalize to entirely new domains with minimal effort.\nThis is the Generalization Gap. Humans are shockingly sample-efficient. A teenager learns to drive not by crashing a million cars in a simulation, but by understanding the physics of the world and applying it. Models, by contrast, require oceans of data to learn what humans grasp intuitively.\nEmotions as “Value Functions” To bridge this gap, Sutskever offered a fascinating technical analogy for human emotion. He suggested that emotions act as a “value function”—a pre-programmed evolutionary guide that helps humans navigate decisions without needing immediate external rewards.\nCurrent RL (Reinforcement Learning) models often need massive amounts of trial-and-error data to learn a task because they lack this internal compass. They essentially have to “crash the car” thousands of times to learn that crashing is bad.\nIf we can discover how to build the equivalent of these “internal value functions” into AI, models could learn much faster and behave more robustly. They would effectively “feel” when an action is wrong or dangerous without needing to be explicitly told, moving us closer to the sample efficiency of biological intelligence.\nEntering the Age of Discovery If the Age of Scaling was about engineering (building bigger pipes for data), the Age of Discovery is about research (figuring out what to put in the pipes).\nSutskever argues we are returning to a dynamic similar to 2012–2020, where progress wasn’t guaranteed by a formula but won through tinkering, insight, and failure.\nKey Characteristics of this New Era:\nIdeas \u003e Compute: While you still need massive compute to train the final system, you don’t need the world’s largest supercomputer to discover the next breakthrough. The Transformer architecture (which powers ChatGPT) was discovered on just 8 to 64 GPUs.\nRisk Returns: Companies can no longer treat AI investment like a treasury bond with fixed yields. Research is inherently uncertain. You have to say, “Go forth, researchers, and find something,” knowing they might come back with nothing.\nBeyond Pre-training: The next leap won’t come from reading more text. It will come from new paradigms—perhaps breakthroughs in how models “reason” during inference (like OpenAI’s o1), or new ways to instill the “values” and robust generalization that humans possess.\nThe Economic Lag Sutskever also touched on a paradox: Why do models crush benchmarks (evals) but struggle to transform the economy instantly?\nHis theory is that we have “over-fit” to benchmarks. By feeding models massive amounts of data related to coding competitions and exams, we’ve created “Student A”—savants who can ace a test but stumble when asked to fix a messy, real-world software bug without introducing two new ones. The economic impact will only catch up when we solve the reliability and generalization problem, not just the test-taking problem.\nConclusion The “Age of Discovery” is exciting, but it is also more dangerous for incumbents. In the Age of Scaling, the winner was whoever had the biggest checkbook. In the Age of Discovery, the winner is whoever has the best ideas.\nAs Sutskever puts it, “Scaling is just one word… but now that compute is big… we are back to the age of research.”\nThe easy growth is over. Now the real science begins.\n",
  "wordCount" : "909",
  "inLanguage": "en",
  "datePublished": "2025-12-07T12:00:00+05:30",
  "dateModified": "2025-12-07T12:00:00+05:30",
  "author":{
    "@type": "Person",
    "name": "Shirley"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chenzheruc.github.io/posts/20251207_ilya/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shirley Blogs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chenzheruc.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenzheruc.github.io/" accesskey="h" title="Shirley Blogs (Alt + H)">Shirley Blogs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://chenzheruc.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      The End of Guaranteed Win: Why Ilya Sutskever Says We&#39;re Entering the Age of Discovery
    </h1>
    <div class="post-meta"><span title='2025-12-07 12:00:00 +0530 +0530'>December 7, 2025</span>&nbsp;·&nbsp;<span>Shirley</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="/pic/20251207/ilya_1.png"></p>
<p>For the last five years, the recipe for AI progress has been remarkably simple: take a neural network, add more data, add more compute, and watch the intelligence line go up. This was the Age of Scaling. It was an era of industrial certainty, where companies could pour billions into hardware with the confidence of a guaranteed return.</p>
<p>But according to Ilya Sutskever, that era is drawing to a close.</p>
<p>In a recent interview with Dwarkesh Patel, Sutskever argued that we are leaving the Age of Scaling (roughly 2020–2025) and entering the Age of Discovery (or Research). Here is why the rules of the game are changing, and what that means for the future of AI.</p>
<h2 id="the-age-of-scaling-when-more-was-enough">The Age of Scaling: When &ldquo;More&rdquo; Was Enough<a hidden class="anchor" aria-hidden="true" href="#the-age-of-scaling-when-more-was-enough">#</a></h2>
<p>Sutskever characterizes the period between 2020 and 2025 as a time when a single insight—pre-training—sucked the air out of the room.</p>
<p>The &ldquo;scaling hypothesis&rdquo; wasn&rsquo;t just a theory; it was a business plan. It told executives exactly what to do: &ldquo;If you mix some compute with some data into a neural net of a certain size, you will get results.&rdquo; It was low-risk and high-reward. Because everyone knew the recipe worked, execution became more important than ideation.</p>
<p>&ldquo;Scaling sucked out all the air in the room,&rdquo; Sutskever noted. &ldquo;We got to the point where there are more companies than ideas.&rdquo;</p>
<p>But this strategy relied on a resource that is now dwindling: pre-training data. The internet is finite. We have scraped the text of humanity, and while synthetic data and &ldquo;souped-up&rdquo; pre-training offer some runway, the era of simply 100x-ing the dataset to get a 100x smarter model is effectively over.</p>
<h2 id="the-competitive-programmer-vs-the-natural-talent">The Competitive Programmer vs. The Natural Talent<a hidden class="anchor" aria-hidden="true" href="#the-competitive-programmer-vs-the-natural-talent">#</a></h2>
<p>To explain why current models fall short despite their massive scale, Sutskever offered a striking analogy involving two students:</p>
<ul>
<li>
<p>Student A (The Model): Wants to be the best competitive programmer. They practice for 10,000 hours, memorize every proof technique, and solve every problem ever written. They become a machine at coding competitions.</p>
</li>
<li>
<p>Student B (The Human): Practices for 100 hours. They have the &ldquo;it&rdquo; factor—a deep, intuitive understanding of the underlying logic.</p>
</li>
</ul>
<p>Currently, LLMs are Student A. They have &ldquo;memorized&rdquo; the internet&rsquo;s worth of patterns (the 10,000 hours), but they lack the fundamental &ldquo;it&rdquo; factor that allows Student B to generalize to entirely new domains with minimal effort.</p>
<p>This is the Generalization Gap. Humans are shockingly sample-efficient. A teenager learns to drive not by crashing a million cars in a simulation, but by understanding the physics of the world and applying it. Models, by contrast, require oceans of data to learn what humans grasp intuitively.</p>
<h2 id="emotions-as-value-functions">Emotions as &ldquo;Value Functions&rdquo;<a hidden class="anchor" aria-hidden="true" href="#emotions-as-value-functions">#</a></h2>
<p>To bridge this gap, Sutskever offered a fascinating technical analogy for human emotion. He suggested that emotions act as a &ldquo;value function&rdquo;—a pre-programmed evolutionary guide that helps humans navigate decisions without needing immediate external rewards.</p>
<p>Current RL (Reinforcement Learning) models often need massive amounts of trial-and-error data to learn a task because they lack this internal compass. They essentially have to &ldquo;crash the car&rdquo; thousands of times to learn that crashing is bad.</p>
<p>If we can discover how to build the equivalent of these &ldquo;internal value functions&rdquo; into AI, models could learn much faster and behave more robustly. They would effectively &ldquo;feel&rdquo; when an action is wrong or dangerous without needing to be explicitly told, moving us closer to the sample efficiency of biological intelligence.</p>
<h2 id="entering-the-age-of-discovery">Entering the Age of Discovery<a hidden class="anchor" aria-hidden="true" href="#entering-the-age-of-discovery">#</a></h2>
<p>If the Age of Scaling was about engineering (building bigger pipes for data), the Age of Discovery is about research (figuring out what to put in the pipes).</p>
<p>Sutskever argues we are returning to a dynamic similar to 2012–2020, where progress wasn&rsquo;t guaranteed by a formula but won through tinkering, insight, and failure.</p>
<p>Key Characteristics of this New Era:</p>
<ul>
<li>
<p>Ideas &gt; Compute: While you still need massive compute to train the final system, you don&rsquo;t need the world&rsquo;s largest supercomputer to discover the next breakthrough. The Transformer architecture (which powers ChatGPT) was discovered on just 8 to 64 GPUs.</p>
</li>
<li>
<p>Risk Returns: Companies can no longer treat AI investment like a treasury bond with fixed yields. Research is inherently uncertain. You have to say, &ldquo;Go forth, researchers, and find something,&rdquo; knowing they might come back with nothing.</p>
</li>
<li>
<p>Beyond Pre-training: The next leap won&rsquo;t come from reading more text. It will come from new paradigms—perhaps breakthroughs in how models &ldquo;reason&rdquo; during inference (like OpenAI&rsquo;s o1), or new ways to instill the &ldquo;values&rdquo; and robust generalization that humans possess.</p>
</li>
</ul>
<h2 id="the-economic-lag">The Economic Lag<a hidden class="anchor" aria-hidden="true" href="#the-economic-lag">#</a></h2>
<p>Sutskever also touched on a paradox: Why do models crush benchmarks (evals) but struggle to transform the economy instantly?</p>
<p>His theory is that we have &ldquo;over-fit&rdquo; to benchmarks. By feeding models massive amounts of data related to coding competitions and exams, we&rsquo;ve created &ldquo;Student A&rdquo;—savants who can ace a test but stumble when asked to fix a messy, real-world software bug without introducing two new ones. The economic impact will only catch up when we solve the reliability and generalization problem, not just the test-taking problem.</p>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>The &ldquo;Age of Discovery&rdquo; is exciting, but it is also more dangerous for incumbents. In the Age of Scaling, the winner was whoever had the biggest checkbook. In the Age of Discovery, the winner is whoever has the best ideas.</p>
<p>As Sutskever puts it, &ldquo;Scaling is just one word&hellip; but now that compute is big&hellip; we are back to the age of research.&rdquo;</p>
<p>The easy growth is over. Now the real science begins.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://chenzheruc.github.io/">Shirley Blogs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
