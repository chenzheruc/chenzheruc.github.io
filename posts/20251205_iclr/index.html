<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The 40-Point Inquisition: When AI Reviews AI at ICLR | Shirley Blogs</title>
<meta name="keywords" content="">
<meta name="description" content="
In the high-stakes world of top-tier AI conferences like ICLR (International Conference on Learning Representations), getting a paper accepted is a career-defining moment. Authors spend months optimizing algorithms, ablation studies, and prose. They expect rigor. They expect tough questions.
They do not expect 40 weaknesses and 40 questions from a single reviewer.
A recent incident involving an ICLR 2026 submission has set the Machine Learning community on fire, highlighting a growing crisis in academic peer review: the suspicion that AI is now reviewing AI, and doing a terrible job of it.">
<meta name="author" content="Shirley">
<link rel="canonical" href="https://chenzheruc.github.io/posts/20251205_iclr/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chenzheruc.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenzheruc.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenzheruc.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenzheruc.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenzheruc.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chenzheruc.github.io/posts/20251205_iclr/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZKM2YW9DPM"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-ZKM2YW9DPM');
        }
      </script><meta property="og:url" content="https://chenzheruc.github.io/posts/20251205_iclr/">
  <meta property="og:site_name" content="Shirley Blogs">
  <meta property="og:title" content="The 40-Point Inquisition: When AI Reviews AI at ICLR">
  <meta property="og:description" content="
In the high-stakes world of top-tier AI conferences like ICLR (International Conference on Learning Representations), getting a paper accepted is a career-defining moment. Authors spend months optimizing algorithms, ablation studies, and prose. They expect rigor. They expect tough questions.
They do not expect 40 weaknesses and 40 questions from a single reviewer.
A recent incident involving an ICLR 2026 submission has set the Machine Learning community on fire, highlighting a growing crisis in academic peer review: the suspicion that AI is now reviewing AI, and doing a terrible job of it.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-12-05T23:00:00+05:30">
    <meta property="article:modified_time" content="2025-12-05T23:00:00+05:30">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The 40-Point Inquisition: When AI Reviews AI at ICLR">
<meta name="twitter:description" content="
In the high-stakes world of top-tier AI conferences like ICLR (International Conference on Learning Representations), getting a paper accepted is a career-defining moment. Authors spend months optimizing algorithms, ablation studies, and prose. They expect rigor. They expect tough questions.
They do not expect 40 weaknesses and 40 questions from a single reviewer.
A recent incident involving an ICLR 2026 submission has set the Machine Learning community on fire, highlighting a growing crisis in academic peer review: the suspicion that AI is now reviewing AI, and doing a terrible job of it.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://chenzheruc.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The 40-Point Inquisition: When AI Reviews AI at ICLR",
      "item": "https://chenzheruc.github.io/posts/20251205_iclr/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The 40-Point Inquisition: When AI Reviews AI at ICLR",
  "name": "The 40-Point Inquisition: When AI Reviews AI at ICLR",
  "description": "\nIn the high-stakes world of top-tier AI conferences like ICLR (International Conference on Learning Representations), getting a paper accepted is a career-defining moment. Authors spend months optimizing algorithms, ablation studies, and prose. They expect rigor. They expect tough questions.\nThey do not expect 40 weaknesses and 40 questions from a single reviewer.\nA recent incident involving an ICLR 2026 submission has set the Machine Learning community on fire, highlighting a growing crisis in academic peer review: the suspicion that AI is now reviewing AI, and doing a terrible job of it.\n",
  "keywords": [
    
  ],
  "articleBody": "\nIn the high-stakes world of top-tier AI conferences like ICLR (International Conference on Learning Representations), getting a paper accepted is a career-defining moment. Authors spend months optimizing algorithms, ablation studies, and prose. They expect rigor. They expect tough questions.\nThey do not expect 40 weaknesses and 40 questions from a single reviewer.\nA recent incident involving an ICLR 2026 submission has set the Machine Learning community on fire, highlighting a growing crisis in academic peer review: the suspicion that AI is now reviewing AI, and doing a terrible job of it.\nThe Incident: “Reviewer #2” on Steroids The drama unfolded when an author took to Reddit to share a bewildering experience. A reviewer had posted a critique containing an exhaustive, seemingly generated list of 40 distinct weaknesses followed by 40 distinct questions.\nFor context, a typical thorough review might list 3-5 major weaknesses and perhaps a handful of clarifying questions. Eighty individual points of contention is not just harsh; it is statistically improbable for a human reviewer to generate without significant redundancy or hallucination.\nThe “Hardware Hallucination” The absurdity of the review became apparent in the specifics. Among the critiques was a demand that the authors demonstrate reproducibility across an exhaustive list of hardware architectures. The reviewer reportedly asked for experiments to be run on Volta, Ampere, Hopper, and Blackwell GPU clusters.\nThis kind of request betrays a fundamental lack of understanding of how academic labs operate (where budget is finite) and suggests a “completionist” logic often found in Large Language Models (LLMs) prompted to be “comprehensive.”\nThe “Inner Calm” Defense When the authors pushed back against this impossible wall of text, the reviewer’s response turned from technical to strangely philosophical. Instead of addressing the scientific rebuttal, the reviewer allegedly replied with:\nIn the current impetuous and intricate society, if one aspires to be a scholar, it is imperative to attain inner calm. Scientific research demands tranquility, particularly peace of mind.\"\nThis phrase, described by community members as reading like a translated excerpt from a “Chinese cultivation novel,” was the smoking gun for many. It suggested the text was either fully generated by an LLM with a specific cultural training bias or heavily translated without context.\nThe Diagnosis: Is it AI? The community consensus is stark: This is almost certainly an AI-generated review.\nSeveral “tells” point to this conclusion:\nStructure: The perfect symmetry (40 weaknesses, 40 questions) suggests a prompt like “List as many weaknesses as possible” or “Generate a comprehensive list of 40 questions.”\nNitpicking: The review obsessed over administrative details, such as copyright licenses for open-source datasets (e.g., demanding specific license types like MIT vs. CC BY-NC for standard datasets), rather than the paper’s core scientific contribution.\nHallucinated Standards: The hardware requirements (testing on unreleased or prohibitively expensive chips) reflect an LLM’s tendency to associate “rigor” with “listing every related entity it knows,” regardless of feasibility.\nThe Broader Problem: The Ouroboros of AI This incident is funny, but it signals a dangerous feedback loop. We have reached a point where:\nResearchers use AI to write code and polish papers.\nReviewers, overwhelmed by the exponential growth of submissions, use AI to generate reviews.\nMeta-reviewers (Area Chairs) might eventually use AI to summarize the AI-generated reviews.\nIf an AI writes a paper and an AI reviews it, does the human understanding matter?\nThe “40 Questions” incident at ICLR 2026 is a wake-up call. It exposes the fragility of the current peer review system, which relies on the good faith and unpaid labor of humans who are increasingly checking out. If conferences cannot detect and filter out low-effort, generated slop—especially slop that demands 40 impossible things before breakfast—the credibility of the field risks collapsing under its own weight.\nFor now, the community watches and waits to see if the Area Chairs will step in, or if we all need to start cultivating “inner calm” to survive the review cycle.\nReferences https://openreview.net/forum?id=kDhAiaGzrn https://openreview.net/forum?id=8qk6eUnvbH https://openreview.net/forum?id=GlXyFjUbfN ",
  "wordCount" : "652",
  "inLanguage": "en",
  "datePublished": "2025-12-05T23:00:00+05:30",
  "dateModified": "2025-12-05T23:00:00+05:30",
  "author":{
    "@type": "Person",
    "name": "Shirley"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chenzheruc.github.io/posts/20251205_iclr/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shirley Blogs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chenzheruc.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenzheruc.github.io/" accesskey="h" title="Shirley Blogs (Alt + H)">Shirley Blogs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://chenzheruc.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <h1 class="post-title entry-hint-parent">
      The 40-Point Inquisition: When AI Reviews AI at ICLR
    </h1>
    <div class="post-meta"><span title='2025-12-05 23:00:00 +0530 +0530'>December 5, 2025</span>&nbsp;·&nbsp;<span>Shirley</span>

</div>
  </header> 
  <div class="post-content"><p><img loading="lazy" src="/pic/20251205/iclr_1.png"></p>
<p>In the high-stakes world of top-tier AI conferences like ICLR (International Conference on Learning Representations), getting a paper accepted is a career-defining moment. Authors spend months optimizing algorithms, ablation studies, and prose. They expect rigor. They expect tough questions.</p>
<p>They do not expect 40 weaknesses and 40 questions from a single reviewer.</p>
<p>A recent incident involving an ICLR 2026 submission has set the Machine Learning community on fire, highlighting a growing crisis in academic peer review: the suspicion that AI is now reviewing AI, and doing a terrible job of it.</p>
<h2 id="the-incident-reviewer-2-on-steroids">The Incident: &ldquo;Reviewer #2&rdquo; on Steroids<a hidden class="anchor" aria-hidden="true" href="#the-incident-reviewer-2-on-steroids">#</a></h2>
<p>The drama unfolded when an author took to Reddit to share a bewildering experience. A reviewer had posted a critique containing an exhaustive, seemingly generated list of 40 distinct weaknesses followed by 40 distinct questions.</p>
<p>For context, a typical thorough review might list 3-5 major weaknesses and perhaps a handful of clarifying questions. Eighty individual points of contention is not just harsh; it is statistically improbable for a human reviewer to generate without significant redundancy or hallucination.</p>
<h3 id="the-hardware-hallucination">The &ldquo;Hardware Hallucination&rdquo;<a hidden class="anchor" aria-hidden="true" href="#the-hardware-hallucination">#</a></h3>
<p>The absurdity of the review became apparent in the specifics. Among the critiques was a demand that the authors demonstrate reproducibility across an exhaustive list of hardware architectures. The reviewer reportedly asked for experiments to be run on Volta, Ampere, Hopper, and Blackwell GPU clusters.</p>
<p>This kind of request betrays a fundamental lack of understanding of how academic labs operate (where budget is finite) and suggests a &ldquo;completionist&rdquo; logic often found in Large Language Models (LLMs) prompted to be &ldquo;comprehensive.&rdquo;</p>
<h3 id="the-inner-calm-defense">The &ldquo;Inner Calm&rdquo; Defense<a hidden class="anchor" aria-hidden="true" href="#the-inner-calm-defense">#</a></h3>
<p>When the authors pushed back against this impossible wall of text, the reviewer’s response turned from technical to strangely philosophical. Instead of addressing the scientific rebuttal, the reviewer allegedly replied with:</p>
<blockquote>
<p>In the current impetuous and intricate society,
if one aspires to be a scholar, it is imperative
to attain inner calm. Scientific research demands
tranquility, particularly peace of mind.&quot;</p></blockquote>
<p>This phrase, described by community members as reading like a translated excerpt from a &ldquo;Chinese cultivation novel,&rdquo; was the smoking gun for many. It suggested the text was either fully generated by an LLM with a specific cultural training bias or heavily translated without context.</p>
<h2 id="the-diagnosis-is-it-ai">The Diagnosis: Is it AI?<a hidden class="anchor" aria-hidden="true" href="#the-diagnosis-is-it-ai">#</a></h2>
<p>The community consensus is stark: This is almost certainly an AI-generated review.</p>
<p>Several &ldquo;tells&rdquo; point to this conclusion:</p>
<ul>
<li>
<p>Structure: The perfect symmetry (40 weaknesses, 40 questions) suggests a prompt like &ldquo;List as many weaknesses as possible&rdquo; or &ldquo;Generate a comprehensive list of 40 questions.&rdquo;</p>
</li>
<li>
<p>Nitpicking: The review obsessed over administrative details, such as copyright licenses for open-source datasets (e.g., demanding specific license types like MIT vs. CC BY-NC for standard datasets), rather than the paper&rsquo;s core scientific contribution.</p>
</li>
<li>
<p>Hallucinated Standards: The hardware requirements (testing on unreleased or prohibitively expensive chips) reflect an LLM&rsquo;s tendency to associate &ldquo;rigor&rdquo; with &ldquo;listing every related entity it knows,&rdquo; regardless of feasibility.</p>
</li>
</ul>
<h2 id="the-broader-problem-the-ouroboros-of-ai">The Broader Problem: The Ouroboros of AI<a hidden class="anchor" aria-hidden="true" href="#the-broader-problem-the-ouroboros-of-ai">#</a></h2>
<p>This incident is funny, but it signals a dangerous feedback loop. We have reached a point where:</p>
<ul>
<li>
<p>Researchers use AI to write code and polish papers.</p>
</li>
<li>
<p>Reviewers, overwhelmed by the exponential growth of submissions, use AI to generate reviews.</p>
</li>
<li>
<p>Meta-reviewers (Area Chairs) might eventually use AI to summarize the AI-generated reviews.</p>
</li>
</ul>
<p>If an AI writes a paper and an AI reviews it, does the human understanding matter?</p>
<p>The &ldquo;40 Questions&rdquo; incident at ICLR 2026 is a wake-up call. It exposes the fragility of the current peer review system, which relies on the good faith and unpaid labor of humans who are increasingly checking out. If conferences cannot detect and filter out low-effort, generated slop—especially slop that demands 40 impossible things before breakfast—the credibility of the field risks collapsing under its own weight.</p>
<p>For now, the community watches and waits to see if the Area Chairs will step in, or if we all need to start cultivating &ldquo;inner calm&rdquo; to survive the review cycle.</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<ul>
<li><a href="https://openreview.net/forum?id=kDhAiaGzrn">https://openreview.net/forum?id=kDhAiaGzrn</a></li>
<li><a href="https://openreview.net/forum?id=8qk6eUnvbH">https://openreview.net/forum?id=8qk6eUnvbH</a></li>
<li><a href="https://openreview.net/forum?id=GlXyFjUbfN">https://openreview.net/forum?id=GlXyFjUbfN</a></li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://chenzheruc.github.io/">Shirley Blogs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
