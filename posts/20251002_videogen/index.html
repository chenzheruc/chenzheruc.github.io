<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The Unseen Architects for Video Generation AI: (1) Training Datasets | Shirley Random Blogs</title>
<meta name="keywords" content="">
<meta name="description" content="Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.

  
      
          Sam Altman in Shanghai
          Sam Altman in Acient China
          Sam Altman talks to a random person
      
  
  
      
          
          
          
      
  

Leading text-to-video generation tools, such as Sora (OpenAI), Veo 3 (Google), Runway, and Kling AI (Kuaishou), are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. For example, OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on &ldquo;internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube&rsquo;s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.">
<meta name="author" content="Shirley">
<link rel="canonical" href="https://chenzheruc.github.io/posts/20251002_videogen/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chenzheruc.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenzheruc.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenzheruc.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenzheruc.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenzheruc.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chenzheruc.github.io/posts/20251002_videogen/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZKM2YW9DPM"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-ZKM2YW9DPM');
        }
      </script><meta property="og:url" content="https://chenzheruc.github.io/posts/20251002_videogen/">
  <meta property="og:site_name" content="Shirley Random Blogs">
  <meta property="og:title" content="The Unseen Architects for Video Generation AI: (1) Training Datasets">
  <meta property="og:description" content="Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.
Sam Altman in Shanghai Sam Altman in Acient China Sam Altman talks to a random person Leading text-to-video generation tools, such as Sora (OpenAI), Veo 3 (Google), Runway, and Kling AI (Kuaishou), are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. For example, OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on “internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube’s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-02T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-02T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Unseen Architects for Video Generation AI: (1) Training Datasets">
<meta name="twitter:description" content="Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.

  
      
          Sam Altman in Shanghai
          Sam Altman in Acient China
          Sam Altman talks to a random person
      
  
  
      
          
          
          
      
  

Leading text-to-video generation tools, such as Sora (OpenAI), Veo 3 (Google), Runway, and Kling AI (Kuaishou), are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. For example, OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on &ldquo;internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube&rsquo;s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://chenzheruc.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The Unseen Architects for Video Generation AI: (1) Training Datasets",
      "item": "https://chenzheruc.github.io/posts/20251002_videogen/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The Unseen Architects for Video Generation AI: (1) Training Datasets",
  "name": "The Unseen Architects for Video Generation AI: (1) Training Datasets",
  "description": "Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.\nSam Altman in Shanghai Sam Altman in Acient China Sam Altman talks to a random person Leading text-to-video generation tools, such as Sora (OpenAI), Veo 3 (Google), Runway, and Kling AI (Kuaishou), are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. For example, OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on \u0026ldquo;internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube\u0026rsquo;s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.\n",
  "keywords": [
    
  ],
  "articleBody": "Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.\nSam Altman in Shanghai Sam Altman in Acient China Sam Altman talks to a random person Leading text-to-video generation tools, such as Sora (OpenAI), Veo 3 (Google), Runway, and Kling AI (Kuaishou), are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. For example, OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on “internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube’s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.\nIn contrast to these guarded, commercial efforts, open research and published technical reports offer valuable insights into the necessary ingredients for high-quality video generation. For instance, detailed data curation and scaling strategies are explored in foundational models such as:\nMovie Gen: A Cast of Media Foundation Models (https://arxiv.org/abs/2410.13720) on Feb 2025 from Meta. HunyuanVideo: A Systematic Framework For Large Video Generative Models (https://arxiv.org/abs/2412.03603) on March 2025 from Tencent. In this article, we deep dive on the data collections according to the two paper above.\nMovie Gen Pre-training Dataset Their pre-training dataset consists of O(100)M video-text pairs and O(1)B image-text pairs.\n",
  "wordCount" : "243",
  "inLanguage": "en",
  "datePublished": "2025-10-02T00:00:00Z",
  "dateModified": "2025-10-02T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Shirley"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chenzheruc.github.io/posts/20251002_videogen/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shirley Random Blogs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chenzheruc.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenzheruc.github.io/" accesskey="h" title="Shirley Random Blogs (Alt + H)">Shirley Random Blogs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://chenzheruc.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      The Unseen Architects for Video Generation AI: (1) Training Datasets
    </h1>
    <div class="post-meta"><span title='2025-10-02 00:00:00 +0000 UTC'>October 2, 2025</span>&nbsp;·&nbsp;<span>Shirley</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#movie-gen" aria-label="Movie Gen">Movie Gen</a><ul>
                        
                <li>
                    <a href="#pre-training-dataset" aria-label="Pre-training Dataset">Pre-training Dataset</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">Sam Altman in Shanghai</th>
          <th style="text-align: center">Sam Altman in Acient China</th>
          <th style="text-align: center">Sam Altman talks to a random person</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><img loading="lazy" src="/pic/sam_altman_1.png"></td>
          <td style="text-align: center"><img loading="lazy" src="/pic/sam_altman_2.png"></td>
          <td style="text-align: center"><img loading="lazy" src="/pic/sam_altman_3.png"></td>
      </tr>
  </tbody>
</table>
<p>Leading text-to-video generation tools, such as Sora (OpenAI), Veo 3 (Google), Runway, and Kling AI (Kuaishou), are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. For example, OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on &ldquo;<a href="https://openai.com/index/video-generation-models-as-world-simulators/">internet-scale data</a>. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube&rsquo;s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.</p>
<p>In contrast to these guarded, commercial efforts, open research and published technical reports offer valuable insights into the necessary ingredients for high-quality video generation. For instance, detailed data curation and scaling strategies are explored in foundational models such as:</p>
<ul>
<li>Movie Gen: A Cast of Media Foundation Models (<a href="https://arxiv.org/abs/2410.13720">https://arxiv.org/abs/2410.13720</a>) on Feb 2025 from Meta.</li>
<li>HunyuanVideo: A Systematic Framework For Large Video Generative Models (<a href="https://arxiv.org/abs/2412.03603">https://arxiv.org/abs/2412.03603</a>) on March 2025 from Tencent.</li>
</ul>
<p>In this article, we deep dive on the data collections according to the two paper above.</p>
<h2 id="movie-gen">Movie Gen<a hidden class="anchor" aria-hidden="true" href="#movie-gen">#</a></h2>
<h3 id="pre-training-dataset">Pre-training Dataset<a hidden class="anchor" aria-hidden="true" href="#pre-training-dataset">#</a></h3>
<p>Their pre-training dataset consists of O(100)M video-text pairs and O(1)B image-text pairs.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://chenzheruc.github.io/">Shirley Random Blogs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
