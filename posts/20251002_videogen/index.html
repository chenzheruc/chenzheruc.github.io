<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>The Unseen Architects for Video Generation AI: Training Data | Shirley Blogs</title>
<meta name="keywords" content="">
<meta name="description" content="Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.

  
      
          In Shanghai
          In Acient China
          Talks to a random person
      
  
  
      
          
          
          
      
  

Sora 2 are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on &ldquo;internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube&rsquo;s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.">
<meta name="author" content="Shirley">
<link rel="canonical" href="https://chenzheruc.github.io/posts/20251002_videogen/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.a090830a421002426baafbd314e38f149d77b4c48a12ee9312700d770b27fb26.css" integrity="sha256-oJCDCkIQAkJrqvvTFOOPFJ13tMSKEu6TEnANdwsn&#43;yY=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://chenzheruc.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://chenzheruc.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://chenzheruc.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://chenzheruc.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://chenzheruc.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://chenzheruc.github.io/posts/20251002_videogen/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
</noscript>
      <script async src="https://www.googletagmanager.com/gtag/js?id=G-ZKM2YW9DPM"></script>
      <script>
        var doNotTrack = false;
        if ( false ) {
          var dnt = (navigator.doNotTrack || window.doNotTrack || navigator.msDoNotTrack);
          var doNotTrack = (dnt == "1" || dnt == "yes");
        }
        if (!doNotTrack) {
          window.dataLayer = window.dataLayer || [];
          function gtag(){dataLayer.push(arguments);}
          gtag('js', new Date());
          gtag('config', 'G-ZKM2YW9DPM');
        }
      </script><meta property="og:url" content="https://chenzheruc.github.io/posts/20251002_videogen/">
  <meta property="og:site_name" content="Shirley Blogs">
  <meta property="og:title" content="The Unseen Architects for Video Generation AI: Training Data">
  <meta property="og:description" content="Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.
In Shanghai In Acient China Talks to a random person Sora 2 are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on “internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube’s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-02T00:00:00+00:00">
    <meta property="article:modified_time" content="2025-10-02T00:00:00+00:00">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="The Unseen Architects for Video Generation AI: Training Data">
<meta name="twitter:description" content="Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.

  
      
          In Shanghai
          In Acient China
          Talks to a random person
      
  
  
      
          
          
          
      
  

Sora 2 are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on &ldquo;internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube&rsquo;s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://chenzheruc.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "The Unseen Architects for Video Generation AI: Training Data",
      "item": "https://chenzheruc.github.io/posts/20251002_videogen/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "The Unseen Architects for Video Generation AI: Training Data",
  "name": "The Unseen Architects for Video Generation AI: Training Data",
  "description": "Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.\nIn Shanghai In Acient China Talks to a random person Sora 2 are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on \u0026ldquo;internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube\u0026rsquo;s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.\n",
  "keywords": [
    
  ],
  "articleBody": "Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.\nIn Shanghai In Acient China Talks to a random person Sora 2 are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on “internet-scale data. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube’s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.\nIn contrast to these guarded, commercial efforts, open research and published technical reports offer valuable insights into the necessary ingredients for high-quality video generation. For instance, detailed data curation and scaling strategies are explored in foundational models such as:\nMovie Gen: A Cast of Media Foundation Models (https://arxiv.org/abs/2410.13720) on Feb 2025 from Meta. HunyuanVideo: A Systematic Framework For Large Video Generative Models (https://arxiv.org/abs/2412.03603) on March 2025 from Tencent. In this article, we deep dive on the data collections according to the two paper above.\nMovie Gen Our pre-training dataset consists of O(100)M video-text pairs and O(1)B image-text pairs.\nOur original pool of data consists of videos that are 4 seconds to 2 minutes long, spanning concepts from different domains such as humans, nature, animals, and objects. Our data curation pipeline yields our final pre-training set of clip-prompt pairs, where each clip is 4s – 16s long, with single-shot camera and non-trivial motion. Our data curation pipeline consists of three filtering stages: 1) visual filtering, 2) motion filtering, and 3) content filtering, and one captioning stage. The filtered clips are annotated with detailed generated captions containing 100 words on average.\nOne more interesting step mentioned is “Multi-stage data curation”, likely due to the intuition of curriculumn learning from easy to hard.\nWe curate 3 subsets of pre-training data with progressively stricter visual, motion, and content thresholds to meet the needs of different stages of pre-training. First, we curated a set of video clips with a minimum width and height of 720 px for low-resolution training. Next, we filtered the set to provide videos with a minimum width and height of 768 px for high-resolution training. Finally, we curated new videos augmenting our high-resolution training set. Our high resolution set has 80% landscape and 20% portrait videos, with at least 60% of them containing humans.\nAnother small finetuning high-quality and manually curated dataset was created, with the goal to improve the video generation quality. Instead of training one model, multiple models will be produced via fine tuning to form the final model with “ensemble” like technique. The extensive manual labor required for cinematic quality control highlights a fundamental, and often debated, trade-off between human oversight and automated scalability in data curation.\nHunyuanVideo To curate the pretraining data, HunyuanVideo follows a similar workflow. Similarly, HunyunVideo curate the multi-stage dataset to support multi-stage training from easy to hard, and they also manually curate a high-quality finetuning dataset to further improve model quality.\nOur hierarchical data filtering pipeline for video data yields five training datasets, corresponding to the five training stages\nwe build a fine-tuning dataset comprising ∼1M samples. This dataset is meticulously curated through human annotation.\nConclusion Ultimately, the power of modern video generation hinges on massive, hyper-detailed datasets. The path forward requires resolving the core tension between the models’ need for near-infinite real-world data and the legal and ethical frameworks restricting its free use. Leveraging real-life data streams captured by emerging technologies like smart glasses presents one potential solution for continuously feeding the AI’s “world simulator.”\nReferences *This article is inspired by Mu Li’s YouTube talk.\n",
  "wordCount" : "634",
  "inLanguage": "en",
  "datePublished": "2025-10-02T00:00:00Z",
  "dateModified": "2025-10-02T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Shirley"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://chenzheruc.github.io/posts/20251002_videogen/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Shirley Blogs",
    "logo": {
      "@type": "ImageObject",
      "url": "https://chenzheruc.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class=" dark" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://chenzheruc.github.io/" accesskey="h" title="Shirley Blogs (Alt + H)">Shirley Blogs</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://chenzheruc.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://chenzheruc.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <h1 class="post-title entry-hint-parent">
      The Unseen Architects for Video Generation AI: Training Data
    </h1>
    <div class="post-meta"><span title='2025-10-02 00:00:00 +0000 UTC'>October 2, 2025</span>&nbsp;·&nbsp;<span>Shirley</span>

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#movie-gen" aria-label="Movie Gen">Movie Gen</a></li>
                <li>
                    <a href="#hunyuanvideo" aria-label="HunyuanVideo">HunyuanVideo</a></li>
                <li>
                    <a href="#conclusion" aria-label="Conclusion">Conclusion</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Following the release of Sora 2 two days ago, Sam Altman has become widely recognized due to his frequent appearances in popular social media videos.</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center">In Shanghai</th>
          <th style="text-align: center">In Acient China</th>
          <th style="text-align: center">Talks to a random person</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><img loading="lazy" src="/pic/20251002/sam_altman_1.png"></td>
          <td style="text-align: center"><img loading="lazy" src="/pic/20251002/sam_altman_2.png"></td>
          <td style="text-align: center"><img loading="lazy" src="/pic/20251002/sam_altman_3.png"></td>
      </tr>
  </tbody>
</table>
<p>Sora 2 are gaining popularity; however, detailed public documentation regarding their underlying training methodologies remains scarce. OpenAI simply noted that Sora takes inspiration from large language models (LLMs) that acquire generalist capabilities by training on &ldquo;<a href="https://openai.com/index/video-generation-models-as-world-simulators/">internet-scale data</a>. It is possible that OpenAI may have scraped YouTube content without permission from Google. On the other hand, Google’s Veo is assumed to benefit from YouTube&rsquo;s high-quality video. The implication is clear: the ability to generate realistic video is directly proportional to access to petabytes of high-quality, varied footage.</p>
<p>In contrast to these guarded, commercial efforts, open research and published technical reports offer valuable insights into the necessary ingredients for high-quality video generation. For instance, detailed data curation and scaling strategies are explored in foundational models such as:</p>
<ul>
<li>Movie Gen: A Cast of Media Foundation Models (<a href="https://arxiv.org/abs/2410.13720">https://arxiv.org/abs/2410.13720</a>) on Feb 2025 from Meta.</li>
<li>HunyuanVideo: A Systematic Framework For Large Video Generative Models (<a href="https://arxiv.org/abs/2412.03603">https://arxiv.org/abs/2412.03603</a>) on March 2025 from Tencent.</li>
</ul>
<p>In this article, we deep dive on the data collections according to the two paper above.</p>
<h2 id="movie-gen">Movie Gen<a hidden class="anchor" aria-hidden="true" href="#movie-gen">#</a></h2>
<blockquote>
<p>Our pre-training dataset consists of O(100)M video-text pairs and O(1)B image-text pairs.</p></blockquote>
<blockquote>
<p>Our original pool of data consists of videos that are 4 seconds to 2 minutes long, spanning concepts from
different domains such as humans, nature, animals, and objects. Our data curation pipeline yields our final
pre-training set of clip-prompt pairs, where each clip is 4s – 16s long, with single-shot camera and non-trivial
motion.
Our data curation pipeline consists of three filtering stages: 1) visual
filtering, 2) motion filtering, and 3) content filtering, and one captioning stage. The filtered clips are annotated
with detailed generated captions containing 100 words on average.</p></blockquote>
<p><img loading="lazy" src="/pic/20251002/videogen_data_flow.png"></p>
<p>One more interesting step mentioned is &ldquo;Multi-stage data curation&rdquo;, likely due to the intuition of curriculumn learning
from easy to hard.</p>
<blockquote>
<p>We curate 3 subsets of pre-training data with progressively stricter visual, motion,
and content thresholds to meet the needs of different stages of pre-training. First, we curated a set of video
clips with a minimum width and height of 720 px for low-resolution training. Next, we filtered the set to
provide videos with a minimum width and height of 768 px for high-resolution training. Finally, we curated
new videos augmenting our high-resolution training set. Our high resolution set has 80% landscape and 20%
portrait videos, with at least 60% of them containing humans.</p></blockquote>
<p>Another small finetuning high-quality and manually curated dataset was created,
with the goal to improve the video generation quality. Instead of training one model,
multiple models will be produced via fine tuning to form the final model with &ldquo;ensemble&rdquo; like technique.
The extensive manual labor required for cinematic quality control highlights a fundamental,
and often debated, trade-off between human oversight and automated scalability in data curation.</p>
<h2 id="hunyuanvideo">HunyuanVideo<a hidden class="anchor" aria-hidden="true" href="#hunyuanvideo">#</a></h2>
<p>To curate the pretraining data, HunyuanVideo follows a similar workflow.
<img loading="lazy" src="/pic/20251002/hunyuan_data_flow.png"></p>
<p>Similarly, HunyunVideo curate the multi-stage dataset to support multi-stage training from easy to hard,
and they also manually curate a high-quality finetuning dataset to further improve model quality.</p>
<blockquote>
<p>Our hierarchical data filtering pipeline for video data yields five training datasets, corresponding
to the five training stages</p></blockquote>
<blockquote>
<p>we build a fine-tuning dataset
comprising ∼1M samples. This dataset is meticulously curated through human annotation.</p></blockquote>
<h2 id="conclusion">Conclusion<a hidden class="anchor" aria-hidden="true" href="#conclusion">#</a></h2>
<p>Ultimately, the power of modern video generation hinges on massive, hyper-detailed datasets. The path forward requires resolving the core tension between the models&rsquo; need for near-infinite real-world data and the legal and ethical frameworks restricting its free use. Leveraging real-life data streams captured by emerging technologies like smart glasses presents one potential solution for continuously feeding the AI&rsquo;s &ldquo;world simulator.&rdquo;</p>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>*This article is inspired by <a href="https://www.youtube.com/watch?v=5MGq7dSOghY">Mu Li&rsquo;s YouTube talk</a>.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2026 <a href="https://chenzheruc.github.io/">Shirley Blogs</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
